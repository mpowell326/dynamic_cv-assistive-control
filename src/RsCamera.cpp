/* -------------------------------------------------------------
    src/RsCamera.cpp        

    Purpose:    Interface to realsense camera for streaming and
                retrieving frames providing them in cv::Mat a
                format.
    
    Created:    Dec 2016
    Author:     Morgan
   ------------------------------------------------------------- */

#include "RsCamera.hpp"





std::vector<uint16_t> enabled_streams = {   (uint16_t)rs::stream::depth                           ,  ///< Native stream of depth data produced by RealSense device
                                            (uint16_t)rs::stream::color                           ,  ///< Native stream of color data captured by RealSense device
                                            (uint16_t)rs::stream::infrared                        ,  ///< Native stream of infrared data captured by RealSense device
                                            (uint16_t)rs::stream::infrared2                       ,  ///< Native stream of infrared data captured from a second viewpoint by RealSense device
                                            // (uint16_t)rs::stream::fisheye                         ,
                                            // (uint16_t)rs::stream::points                          ,  ///< Synthetic stream containing point cloud data generated by deprojecting the depth image
                                            // (uint16_t)rs::stream::rectified_color                 ,  ///< Synthetic stream containing undistorted color data with no extrinsic rotation from the depth stream
                                            // (uint16_t)rs::stream::color_aligned_to_depth          ,  ///< Synthetic stream containing color data but sharing intrinsic of depth stream
                                            // (uint16_t)rs::stream::infrared2_aligned_to_depth      ,  ///< Synthetic stream containing second viewpoint infrared data but sharing intrinsic of depth stream
                                            // (uint16_t)rs::stream::depth_aligned_to_color          ,  ///< Synthetic stream containing depth data but sharing intrinsic of color stream
                                            // (uint16_t)rs::stream::depth_aligned_to_rectified_color, ///< Synthetic stream containing depth data but sharing intrinsic of rectified color stream
                                            // (uint16_t)rs::stream::depth_aligned_to_infrared2      ,  ///< Synthetic stream containing depth data but sharing intrinsic of second viewpoint infrared stream
                                        };

std::vector<uint16_t> displayed_streams = { (uint16_t)rs::stream::depth                           ,  ///< Native stream of depth data produced by RealSense device
                                            (uint16_t)rs::stream::color                           ,  ///< Native stream of color data captured by RealSense device
                                            // (uint16_t)rs::stream::infrared                        ,  ///< Native stream of infrared data captured by RealSense device
                                            // (uint16_t)rs::stream::infrared2                       ,  ///< Native stream of infrared data captured from a second viewpoint by RealSense device
                                            // (uint16_t)rs::stream::fisheye                         ,
                                            // (uint16_t)rs::stream::points                          ,  ///< Synthetic stream containing point cloud data generated by deprojecting the depth image
                                            // (uint16_t)rs::stream::rectified_color                 ,  ///< Synthetic stream containing undistorted color data with no extrinsic rotation from the depth stream
                                            // (uint16_t)rs::stream::color_aligned_to_depth          ,  ///< Synthetic stream containing color data but sharing intrinsic of depth stream
                                            // (uint16_t)rs::stream::infrared2_aligned_to_depth      ,  ///< Synthetic stream containing second viewpoint infrared data but sharing intrinsic of depth stream
                                            // (uint16_t)rs::stream::depth_aligned_to_color          ,  ///< Synthetic stream containing depth data but sharing intrinsic of color stream
                                            // (uint16_t)rs::stream::depth_aligned_to_rectified_color, ///< Synthetic stream containing depth data but sharing intrinsic of rectified color stream
                                            // (uint16_t)rs::stream::depth_aligned_to_infrared2      ,  ///< Synthetic stream containing depth data but sharing intrinsic of second viewpoint infrared stream
                                        };


// r200_lr_auto_exposure_enabled                   , /**< Enables / disables R200 auto-exposure. This will affect both IR and depth image.*/
// r200_lr_gain                                    , /**< IR image gain*/
// r200_lr_exposure                                , /**< This control allows manual adjustment of the exposure time value for the L/R imagers*/
// r200_emitter_enabled                            , /**< Enables / disables R200 emitter*/
// r200_depth_units                                , /**< Micrometers per increment in integer depth values, 1000 is default (mm scale). Set before streaming*/
// r200_depth_clamp_min                            , /**< Minimum depth in current depth units that will be output. Any values less than ‘Min Depth’ will be mapped to 0 during the conversion between disparity and depth. Set before streaming*/
// r200_depth_clamp_max                            , /**< Maximum depth in current depth units that will be output. Any values greater than ‘Max Depth’ will be mapped to 0 during the conversion between disparity and depth. Set before streaming*/
// r200_disparity_multiplier                       , /**< The disparity scale factor used when in disparity output mode. Can only be set before streaming*/
// r200_disparity_shift                            , /**< {0 - 512}. Can only be set before streaming starts*/
// r200_auto_exposure_mean_intensity_set_point     , /**< (Requires LR-Auto-Exposure ON) Mean intensity set point*/
// r200_auto_exposure_bright_ratio_set_point       , /**< (Requires LR-Auto-Exposure ON) Bright ratio set point*/
// r200_auto_exposure_kp_gain                      , /**< (Requires LR-Auto-Exposure ON) Kp Gain*/
// r200_auto_exposure_kp_exposure                  , /**< (Requires LR-Auto-Exposure ON) Kp Exposure*/
// r200_auto_exposure_kp_dark_threshold            , /**< (Requires LR-Auto-Exposure ON) Kp Dark Threshold*/
// r200_auto_exposure_top_edge                     , /**< (Requires LR-Auto-Exposure ON) Auto-Exposure region-of-interest top edge (in pixels)*/
// r200_auto_exposure_bottom_edge                  , /**< (Requires LR-Auto-Exposure ON) Auto-Exposure region-of-interest bottom edge (in pixels)*/
// r200_auto_exposure_left_edge                    , /**< (Requires LR-Auto-Exposure ON) Auto-Exposure region-of-interest left edge (in pixels)*/
// r200_auto_exposure_right_edge                   , /**< (Requires LR-Auto-Exposure ON) Auto-Exposure region-of-interest right edge (in pixels)*/
// r200_depth_control_estimate_median_decrement    , /**< Value to subtract when estimating the median of the correlation surface*/
// r200_depth_control_estimate_median_increment    , /**< Value to add when estimating the median of the correlation surface*/
// r200_depth_control_median_threshold             , /**< A threshold by how much the winning score must beat the median*/
// r200_depth_control_score_minimum_threshold      , /**< The minimum correlation score that is considered acceptable*/
// r200_depth_control_score_maximum_threshold      , /**< The maximum correlation score that is considered acceptable*/
// r200_depth_control_texture_count_threshold      , /**< A parameter for determining whether the texture in the region is sufficient to justify a depth result*/
// r200_depth_control_texture_difference_threshold , /**< A parameter for determining whether the texture in the region is sufficient to justify a depth result*/
// r200_depth_control_second_peak_threshold        , /**< A threshold on how much the minimum correlation score must differ from the next best score*/
// r200_depth_control_neighbor_threshold           , /**< Neighbor threshold value for depth calculation*/
// r200_depth_control_lr_threshold                 , /**< Left-Right threshold value for depth calculation*/



void RsCamera::setParams()
{
    /* Get Rs options */
    cout<<"Option"<<endl;
    double min, max, step, current;
    _device->get_option_range(rs::option::r200_lr_gain, min, max, step, current);
    cout<<"Min: "<<min<<", Max: "<<max<<", Step: "<<step<<", Default: "<<current<<endl;

    
    /* Set Rs options */
    // rs::apply_depth_control_preset(_device, 3);   
    // _device->set_option(rs::option::r200_emitter_enabled,1);
    // _device->set_option(rs::option::r200_lr_gain,2000);
    // _device->set_option(rs::option::r200_depth_units,10000);

    // _device->set_option(rs::option::r200_lr_auto_exposure_enabled,1);
    // _device->set_option(rs::option::r200_depth_clamp_min,5);

    /* Settings for outdoors:
    referenced from: https://github.com/IntelRealSense/librealsense/issues/208
    */
    // _device->set_option(rs::option::color_backlight_compensation,0);
    // _device->set_option(rs::option::color_brightness, 62);
    // _device->set_option(rs::option::color_contrast, 38 );
    // _device->set_option(rs::option::color_enable_auto_white_balance, 0 );
    // _device->set_option(rs::option::color_gain, 88 );
    // _device->set_option(rs::option::color_gamma, 180 );
    // _device->set_option(rs::option::color_hue, -1018 );
    // _device->set_option(rs::option::color_saturation, 87 );
    // _device->set_option(rs::option::color_sharpness, 7 );
    // _device->set_option(rs::option::color_white_balance, 2987 );
    // _device->set_option(rs::option::r200_lr_auto_exposure_enabled, 1); 
    // _device->set_option(rs::option::r200_auto_exposure_bottom_edge, 239); 
    // _device->set_option(rs::option::r200_auto_exposure_left_edge, 0);
    // _device->set_option(rs::option::r200_auto_exposure_right_edge, 319); 
    // _device->set_option(rs::option::r200_auto_exposure_top_edge, 0); 
    // _device->set_option(rs::option::r200_emitter_enabled, 1);
    // _device->set_option(rs::option::r200_lr_exposure, 7); 
    // _device->set_option(rs::option::r200_lr_gain, 100);




    // _device->set_option(rs::option::r200_lr_auto_exposure_enabled                   , 1); 
    // _device->set_option(rs::option::r200_lr_gain                                    , 100);
    // _device->set_option(rs::option::r200_lr_exposure                                , 164); 
    // _device->set_option(rs::option::r200_emitter_enabled                            , 1);
    // _device->set_option(rs::option::r200_depth_units                                , 1000);
    // _device->set_option(rs::option::r200_depth_control_estimate_median_decrement    , 43);
    // _device->set_option(rs::option::r200_depth_control_estimate_median_increment    , 0);
    // _device->set_option(rs::option::r200_depth_control_median_threshold             , 0);
    // _device->set_option(rs::option::r200_depth_control_score_minimum_threshold      , 0);
    // _device->set_option(rs::option::r200_depth_control_score_maximum_threshold      , 555);
    // _device->set_option(rs::option::r200_depth_control_texture_count_threshold      , 6);
    // _device->set_option(rs::option::r200_depth_control_texture_difference_threshold , 14);
    // _device->set_option(rs::option::r200_depth_control_second_peak_threshold        , 19);
    // _device->set_option(rs::option::r200_depth_control_neighbor_threshold           , 0);
    // _device->set_option(rs::option::r200_depth_control_lr_threshold                 , 9); 

}

bool RsCamera::startStreaming()
{
    bool success = false;


    printf("There are %d connected RealSense devices.\n", _rsCtx.get_device_count());
    if(_rsCtx.get_device_count() != 0)
    {
        _device = _rsCtx.get_device(0);
        // cout<<enabled_streams.size()<<endl;
        streams_mat.resize(12);
        // rsCloudPtr = cloud(new pcl::PointCloud<pcl::PointXYZRGB>);

        printf("\nUsing device 0, an %s\n", _device->get_name());
        printf("    Serial number: %s\n", _device->get_serial());
        printf("    Firmware version: %s\n", _device->get_firmware_version());


        /* Configure all streams */
        rs::preset stream_preset = rs::preset::highest_framerate;   //best_quality, largest_image, highest_framerate

        // _device->enable_stream(rs::stream::depth, 0, 0, rs::format::z16, fps, rs::output_buffer_format::native);
        // _device->enable_stream(rs::stream::color, frameWidth, frameHeight, rs::format::rgb8, fps, rs::output_buffer_format::native);
        // _device->enable_stream(rs::stream::infrared, 0, 0, rs::format::y8, fps, rs::output_buffer_format::native);
        // if (_device->supports(rs::capabilities::infrared2))
        //     _device->enable_stream(rs::stream::infrared2, 0, 0, rs::format::y8, fps, rs::output_buffer_format::native);

        _device->enable_stream(rs::stream::depth, stream_preset);
        _device->enable_stream(rs::stream::color, stream_preset);
        _device->enable_stream(rs::stream::infrared, stream_preset);
        if (_device->supports(rs::capabilities::infrared2))
            _device->enable_stream(rs::stream::infrared2, stream_preset);

        
        setParams();
        _device->start();

        success = true;
    }
    return success;
}

void RsCamera::stopStreaming()
{
    _device->stop();

    for (auto i : enabled_streams)
    {
        if (_device->is_stream_enabled((rs::stream)i))
            _device->disable_stream((rs::stream)i);
    }
}


void RsCamera::getNextFrame()
{
    const void * rawFrame;
    const double timestamp = _device->get_frame_timestamp(rs::stream::color);


    _device->wait_for_frames();

    if(timestamp != last_timestamp)
    {
        /* ==== Data Grab ==== */
        assert( generatePointCloud(rsCloudPtr) == EXIT_SUCCESS );
            

        for (auto i : enabled_streams)
        {
            rawFrame = _device->get_frame_data( (rs::stream)i );
            convertRsFrame2Mat( (rs::stream)i, rawFrame , &streams_mat[i] );
        }
        if (HDR_ENABLED)
        {
            cv::Mat depthShadows;
            cv::Mat depthHighlights;
            // cv::Mat temp;

            // Get depth image with low exposure(shadows):
            _device->set_option(rs::option::r200_lr_exposure, 7); 
            _device->wait_for_frames();
            rawFrame = _device->get_frame_data( rs::stream::depth );
            convertRsFrame2Mat( rs::stream::depth, rawFrame , &depthShadows );
            cvWaitKey( 50 );
            // Get depth image with high exposure(highlights):
            _device->set_option(rs::option::r200_lr_exposure, 164); 
            _device->wait_for_frames();
            rawFrame = _device->get_frame_data( rs::stream::depth );
            convertRsFrame2Mat( rs::stream::depth, rawFrame , &depthHighlights );
            cvWaitKey( 50 );
            // streams_mat[0] = streams_mat[0];
            // streams_mat[1] = depthShadows;
            // streams_mat[2] = depthHighlights;

            streams_mat[0].push_back(depthShadows);
            streams_mat[0].push_back(depthHighlights);

            _device->set_option(rs::option::r200_lr_exposure, 60);
            cvWaitKey( 50 );
        }
        last_timestamp = timestamp;
        ++num_frames;
        if(timestamp >= next_time)
        {
            currentFps = num_frames;
            num_frames = 0;
            next_time += 1000;
        }
    }
}
 
void RsCamera::convertRsFrame2Mat(rs::stream stream, const void * data, cv::Mat *outImg)
{
    int cvDataType;
    int cvDataWidth;


    rs::format streamFormat = _device->get_stream_format( stream );
    // cout << "Stream format: " << streamFormat <<endl;
    int h = _device->get_stream_height(stream);
    int w = _device->get_stream_width(stream);

    switch (streamFormat)
    {
        case rs::format::any:
            throw std::runtime_error("not a valid format");
            break;
        case rs::format::xyz32f:            //< 32 bit floating point 3D coordinates.
            cvDataType = CV_32FC3;
            cvDataWidth = 12;               /* <<<---- NOTE: needs fixed !  */
            outImg->create( h, w, cvDataType);
            memcpy(outImg->data, (uchar*)reinterpret_cast<const rs::float3 *>(data), h*w*cvDataWidth);
            break;

        /* STREAM_TYPE_DEPTH */
        case rs::format::z16:               //< 16 bit linear depth values. The depth in meters is equal to depth scale * pixel value
            // const uint16_t one_meter = (uint16_t)(1.0f / _device->get_depth_scale());

            // cvDataType = CV_16U;
            // cvDataWidth = 2;    
            // outImg->create( h, w, cvDataType);
            // memcpy(outImg->data, reinterpret_cast<const uint16_t*>(data), h*w*cvDataWidth);
            // outImg->convertTo( (* outImg), CV_8UC1, 255.0f/2.0f* _device->get_depth_scale() );
            // break;

        case rs::format::disparity16:       //< 16 bit linear disparity values. The depth in meters is equal to depth scale / pixel value
            cvDataType = CV_16U;
            cvDataWidth = 2;    
            outImg->create( h, w, cvDataType);
            memcpy(outImg->data, reinterpret_cast<const uint16_t*>(data), h*w*cvDataWidth);
            outImg->convertTo( (* outImg), CV_8UC1, 255.0f/2.0f* _device->get_depth_scale() );
            break;

        /* STREAM_TYPE_COLOR */
        case rs::format::rgb8:
        case rs::format::bgr8:
            cvDataType = CV_8UC3;
            cvDataWidth = 3;
            outImg->create( h, w, cvDataType);
            memcpy(outImg->data, (uchar*)reinterpret_cast<const uint8_t *>(data), h*w*cvDataWidth);
            cv::cvtColor( (* outImg), (* outImg), cv::COLOR_BGR2RGB );
            break;
        case rs::format::rgba8:
        case rs::format::bgra8:   
            cvDataType = CV_8UC4;
            cvDataWidth = 4;
            outImg->create( h, w, cvDataType);
            memcpy(outImg->data, data, h*w*cvDataWidth);
            break;
        case rs::format::yuyv:
        case rs::format::raw10:             //< Four 10-bit luminance values encoded into a 5-byte macropixel
        case rs::format::raw16:             //< Four 10-bit luminance filled in 16 bit pixel (6 bit unused)
            throw(0); // Not implemented
            break;
        
        /* STREAM_TYPE_IR */
        case rs::format::y8:                /* Relative IR Image */
            cvDataType = CV_8U;
            cvDataWidth = 1;
            outImg->create( h, w, cvDataType);
            memcpy(outImg->data, data, h*w*cvDataWidth);
            break;
        case rs::format::y16:               /* 16-Bit Gray Image */
            cvDataType = CV_16U;
            cvDataWidth = 2;
            outImg->create( h, w, cvDataType);
            memcpy(outImg->data, data, h*w*cvDataWidth);
            break;

        /* Monochrome Wide FOW */
        case rs::format::raw8:
            throw(0); // Not implemented
            break;

        default:
            throw std::runtime_error("The requested format is not provided by demo");
            break;
    }


    // auto points = reinterpret_cast<const rs::float3 *>(_device.get_frame_data(rs::stream::points));
    //     auto depth = reinterpret_cast<const uint16_t *>(_device.get_frame_data(rs::stream::depth));
        
    //     for(int y=0; y<depth_intrin.height; ++y)
    //     {
    //         for(int x=0; x<depth_intrin.width; ++x)
    //         {
    //             if(points->z) //if(uint16_t d = *depth++)
    //             {
    //                 //const rs::float3 point = depth_intrin.deproject({static_cast<float>(x),static_cast<float>(y)}, d*depth_scale);
    //                 glTexCoord(identical ? tex_intrin.pixel_to_texcoord({static_cast<float>(x),static_cast<float>(y)}) : tex_intrin.project_to_texcoord(extrin.transform(*points)));
    //                 glVertex(*points);
    //             }
    //             ++points;
    //         }
    //     }
}


/*===================================================================
   Get the raw data and build the current frames cloud

 *  Created on: Oct 24, 2016
 *      Author: rick

  =================================================================== */
int RsCamera::generatePointCloud( pcl::PointCloud<pcl::PointXYZRGB>::Ptr rs_cloud_ptr )
{

    // Wait for new frame data
    // if( _device->is_streaming( ) )
    //     _device->wait_for_frames();

    // Retrieve our images
    const uint16_t * depth_image    = ( const uint16_t * )_device->get_frame_data( rs::stream::depth );
    const uint8_t  * color_image    = ( const uint8_t  * )_device->get_frame_data( rs::stream::color );

    // Retrieve camera parameters for mapping between depth and color
    rs::intrinsics depth_intrin     = _device->get_stream_intrinsics( rs::stream::depth );
    rs::extrinsics depth_to_color   = _device->get_extrinsics( rs::stream::depth, rs::stream::color );
    rs::intrinsics color_intrin     = _device->get_stream_intrinsics( rs::stream::color );
    float scale                     = _device->get_depth_scale( );

    // Depth dimension helpers
    int dw  = 0;
    int dh  = 0;
    int dwh = 0;

    dw = depth_intrin.width;
    dh = depth_intrin.height;

    dwh = dw * dh;

    // Set the cloud up to be used
    rs_cloud_ptr->clear( );
    rs_cloud_ptr->is_dense = false;
    rs_cloud_ptr->resize( dwh );

    // Iterate the data space
    // First, iterate across columns
    for( int dy = 0; dy < dh; dy++ )
    {

        // Second, iterate across rows
        for( int dx = 0; dx < dw; dx++ )
        {
            uint i = dy * dw + dx;
            uint16_t depth_value = depth_image[ i ];

            if( depth_value == 0 )
                continue;

            rs::float2 depth_pixel = { (float)dx, (float)dy };
            float depth_in_meters = depth_value * scale;

            rs::float3 depth_point = depth_intrin.deproject( depth_pixel, depth_in_meters );
            rs::float3 color_point = depth_to_color.transform(depth_point);
            rs::float2 color_pixel = color_intrin.project(color_point);

            const int cx = ( int )std::round( color_pixel.x );
            const int cy = ( int )std::round( color_pixel.y );

            static const float nan = std::numeric_limits<float>::quiet_NaN( );

            // Set up logic to remove bad points
            bool depth_fail = true;
            bool color_fail = true;

            depth_fail = ( depth_point.z > NOISY_DISTANCE );
            color_fail = ( cx < 0 || cy < 0 || cx > color_intrin.width || cy > color_intrin.height );

            // ==== Cloud Input Pointers ====

            // XYZ input access to cloud
            float *dp_x;
            float *dp_y;
            float *dp_z;

            dp_x = &( rs_cloud_ptr->points[ i ].x );
            dp_y = &( rs_cloud_ptr->points[ i ].y );
            dp_z = &( rs_cloud_ptr->points[ i ].z );

            // RGB input access to cloud
            uint8_t *cp_r;
            uint8_t *cp_g;
            uint8_t *cp_b;

            cp_r = &( rs_cloud_ptr->points[ i ].r );
            cp_g = &( rs_cloud_ptr->points[ i ].g );
            cp_b = &( rs_cloud_ptr->points[ i ].b );

            // ==== Cloud Input Data ====
            // Set up depth point data
            float real_x        = 0;
            float real_y        = 0;
            float real_z        = 0;
            float adjusted_x    = 0;
            float adjusted_y    = 0;
            float adjusted_z    = 0;

            real_x = depth_point.x;
            real_y = depth_point.y;
            real_z = depth_point.z;

            // Adjust point to coordinates
            adjusted_x = -1 * real_x;
            adjusted_y = -1 * real_y;
            adjusted_z = real_z;

            // Set up color point data
            const uint8_t *offset = ( color_image + ( cy * color_intrin.width + cx ) * 3 );

            uint8_t raw_r       = 0;
            uint8_t raw_g       = 0;
            uint8_t raw_b       = 0;
            uint8_t adjusted_r  = 0;
            uint8_t adjusted_g  = 0;
            uint8_t adjusted_b  = 0;
            // cout<<"here8"<<endl;
            if( depth_fail || color_fail )
            {
                *dp_x = *dp_y = *dp_z = (float) nan;
                *cp_r = *cp_g = *cp_b = 0;
                continue;
            }
            raw_r = *( offset );
            raw_g = *( offset + 1 );
            raw_b = *( offset + 2 );

            // Adjust color arbitrarily
            adjusted_r = raw_r;
            adjusted_g = raw_g;
            adjusted_b = raw_b;

            // ==== Cloud Point Evaluation ====
            // If bad point, remove & skip
            if( depth_fail || color_fail )
            {
                *dp_x = *dp_y = *dp_z = (float) nan;
                *cp_r = *cp_g = *cp_b = 0;
                continue;
            }

            // If valid point, add data to cloud
            else
            {
                // Fill in cloud depth
                *dp_x = adjusted_x;
                *dp_y = adjusted_y;
                *dp_z = adjusted_z;

                // Fill in cloud color
                *cp_r = adjusted_r;
                *cp_g = adjusted_g;
                *cp_b = adjusted_b;
            }
        }
    }
    return EXIT_SUCCESS;
}


int RsCamera::getFps()
{
    return currentFps;
}


cv::Mat* RsCamera::getMat(rs::stream stream)
{
    return &streams_mat[(int)stream];
}






//===================================================================
// Create the viewer window where the point cloud is rendered
//===================================================================

    

PCLViewer::PCLViewer()
{
    pclVisualizer   = createPointCloudWindow();
    plotter         = createPlotterWindow();
}


std::shared_ptr<pcl::visualization::PCLVisualizer> PCLViewer::createPointCloudWindow()
{
    // Open 3D viewer and add point cloud
    std::shared_ptr<pcl::visualization::PCLVisualizer> viewer( new pcl::visualization::PCLVisualizer( "LibRealSense PCL Viewer" ) );

    viewer->setBackgroundColor( 0.251, 0.251, 0.251 ); // Floral white 1, 0.98, 0.94 | Misty Rose 1, 0.912, 0.9 |
    viewer->addCoordinateSystem( 0.05 );
    viewer->initCameraParameters( );
    viewer->setShowFPS( true );

    return( viewer );
}


std::shared_ptr<pcl::visualization::PCLPlotter> PCLViewer::createPlotterWindow()
{
    /* === Setup Plotter ==== */
    std::shared_ptr<pcl::visualization::PCLPlotter> plotter( new pcl::visualization::PCLPlotter () );
    // plotter->setXRange (-40.0, 40.0);
    plotter->setXRange (-2.0, 2.0);
    plotter->setYRange (0, MAX_RANGE);
    plotter->setXTitle("Angle (deg)");
    plotter->setYTitle("Distance (m)");

    return( plotter );
}


template void PCLViewer::addRGBCloud<pcl::PointXYZRGB>  (  typename pcl::PointCloud<pcl::PointXYZRGB>::Ptr  cloud, const char* id, int size );
template <typename PointT> void PCLViewer::addRGBCloud(typename pcl::PointCloud<PointT>::Ptr cloud, const char* id, int size )
{
    pointCloudsRGB.push_back( std::pair<std::string, pcl::PointCloud<pcl::PointXYZRGB>::ConstPtr> (id, cloud));
    pclVisualizer->addPointCloud( cloud, id );
    pclVisualizer->setPointCloudRenderingProperties( pcl::visualization::PCL_VISUALIZER_POINT_SIZE, size, id); 
}


template void PCLViewer::addXYZCloud<pcl::PointXYZ>     (  typename pcl::PointCloud<pcl::PointXYZ>::Ptr     cloud, const char* id, int size, int r, int g, int b );
template <typename PointT> void PCLViewer::addXYZCloud(typename pcl::PointCloud<PointT>::Ptr cloud, const char* id, int size, int r, int g, int b )
{
    pointCloudsXYZ.push_back( std::pair<std::string, pcl::PointCloud<pcl::PointXYZ>::ConstPtr> (id, cloud));
    pclVisualizer->addPointCloud( cloud, id );
    pclVisualizer->setPointCloudRenderingProperties( pcl::visualization::PCL_VISUALIZER_COLOR, r,g,b, id );
    pclVisualizer->setPointCloudRenderingProperties( pcl::visualization::PCL_VISUALIZER_POINT_SIZE, size, id);    

}


void PCLViewer::updatePlot(std::vector<std::pair<double, double>> &obstacles)
{
    plotter->clearPlots();
    plotter->addPlotData (obstacles,"Distance to Osbtacle",vtkChart::POINTS);
}

void PCLViewer::updatePlot(std::vector<std::vector<std::pair<double, double>>> &obstacles)
{
    plotter->clearPlots();
    for( int i=0; i < obstacles.size(); i++ )
    {
        plotter->addPlotData (obstacles[i],"Distance to Osbtacle",vtkChart::POINTS);
    }
}


void PCLViewer::display()
{
    for( int i = 0; i < pointCloudsRGB.size(); i++ )
    {
        pclVisualizer->updatePointCloud( pointCloudsRGB[i].second, pointCloudsRGB[i].first );
    }

    for( int i = 0; i < pointCloudsXYZ.size(); i++ )
    {
        pclVisualizer->updatePointCloud( pointCloudsXYZ[i].second, pointCloudsXYZ[i].first );
    }

    plotter->spinOnce( 1 );
    pclVisualizer->spinOnce( 1 );
}
    

PCLViewer::~PCLViewer()
{
    pclVisualizer->close( );
}













//===================================================================
// Viewer to display OpenCV type images
//===================================================================


Displayer::Displayer(RsCamera* rsCamera)
{
    camera = rsCamera;
}

void Displayer::initializeWindows()
{
    for (auto i : displayed_streams)
    {
        cv::namedWindow( std::to_string(  i ) , 0 );
    }
}


void Displayer::displayFps(int fps)
{
    cout << "FPS: " << fps << endl;
}


void Displayer::displayStreams()
{
    cv::Mat tmp;

    for (auto i : displayed_streams)
    {
        tmp = (* camera->getMat((rs::stream)i));

        if( (rs::stream)i == rs::stream::depth )
        {
            convertDepthMat4Display( &tmp, &tmp );
        }
        // if( (rs::stream)i == rs::stream::points )
        // {

        // }
        cv::imshow( std::to_string(i), tmp );
    }
    cvWaitKey( 1 );

}


void Displayer::displayMat(cv::Mat* frame, char* name)
{

    cv::imshow( name, (* frame) );
}


void Displayer::convertDepthMat4Display(cv::Mat* input, cv::Mat* output)
{
    input->convertTo( (* output), CV_8UC1);
    applyColorMap((* output), (* output), cv::COLORMAP_WINTER);
}



